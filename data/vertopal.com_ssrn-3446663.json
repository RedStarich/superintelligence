{"\ufeff\nThe State of Research in Existential Risk \nSe\u00e1n S. \u00d3h\u00c9igeartaigh \nCentre for the Study of Existential Risk,University of Cambridge \nso348@cam.ac.uk\nIn B.J. Garrick (Editor), Catastrophic and Existential Risk: Proceedings of the First Colloquium, sponsored by The B. John Garrick Institute for the Risk Sciences, Henry Samueli School of Engineering and Applied Science, University of California, Los Angeles, UCLA Luskin Convention Center, March 27-29, 2017. This version 16 August 2017. \nAbstract \nIn the last fifteen years there has been substantial growth in research on existential risk \u2013 the category of risks that threaten human extinction, or the permanent and drastic reduction of humanity\u2019s future potential. A number of new organisations focused explicitly on existential and global catastrophic risk have been founded in recent years, complementing the long-standing work of existing centres focused on specific risk areas such as nuclear war, biosecurity, climate change and systemic risk. This paper provides a brief overview of the emergence of this new research community, and provides a case study on the community\u2019s research on potential risks posed by future developments in artificial intelligence. There exists the opportunity for powerful collaboration between the new approaches and perspectives provided by the existential risk research community, and the expertise and tools developed by the risk sciences for risks of various magnitudes. However, there are a number of key characteristics of existential and global catastrophic risks, such as their magnitude, and their rare or unprecedented nature, that are likely to make them particularly challenging to submit to standard risk analysis, and will require new and specialised approaches. \nExistential and Global Catastrophic Risk \nAn existential risk is one that threatens the premature extinction of Earth-originating intelligent life, or the permanent and drastic destruction of its potential for desirable future development (Bostrom, 2002). For most practical purposes, this refers to developments that might wipe out humanity, or lock us into a situation we (or other intelligent life on earth) cannot recover from, such as a major and permanent global civilizational collapse.1\nThe Centre for the Study of Existential Risk, among other organisations within the existential risk research community, also includes a focus on global catastrophic risks. Here, a common definition is that used by the Global Catastrophic Risk Institute: \nGlobal catastrophic risk is the risk of events large enough to significantly harm or even destroy human civilization at the global scale\u201d ((http://gcrinstitute.org/concept). \nFor this definition of global catastrophic risk, human extinction risks would be a subset of a broader set of global catastrophic risks. However, precise definitions for these classes of risk are less important than having a shared sense of the magnitude of scope of events or developments under consideration, \n1 It is worth noting that Bostrom also considers some more unusual possibilities, such as the emergence of permanent totalitarianism \nElectronic copy available at: https://ssrn.com/abstract": "3446663\n\nand the key characteristics of such events or developments. The study of existential and global catastrophic risk restricts us to events or trends that might lead to a full civilizational collapse. It rules out localised catastrophes, and global-scale events that would represent a tragedy but that would not impact our civilisation in the longer-term \u2013 unless these events were likely to play a key role in more severe and permanent cascades and collapses. \nEvents as historically significant as Chernobyl, Hurricane Katrina, the Ebola outbreak, most of our wars in the 20th century, and even the Spanish influenza would fail to constitute global catastrophes or existential threats. For this research community, their primary relevance is in what they can tell us about more severe possibilities within the relevant risk categories. On the other hand, various near-misses during the Cold War (Lewis et al, 2014) might plausibly have led to a global thermonuclear war with global catastrophic consequences, and thus this topic is firmly in scope. \nA clear example of an existential risk is the risk of an asteroid impact on the scale of that which wiped out the dinosaurs 66 million years ago (Schulte et al, 2010). An example of a global catastrophic risk that is not existential might include a large-scale pandemic disease outbreak (Palmer et al, 2017", " Millett & Snyder-Beattie, 2017). Research and expert opinion indicates that it is unlikely that a natural pandemic outbreak could wipe out all humans across the globe given our distribution, immune variation, and other factors": null, " and in most plausible scenarios global recovery seems likely in the longer term. \nIt is also important to consider factors that increase stress or resilience on a global scale \u2013 events or developments that in of themselves would not be a global catastrophe, but might make it more or less likely for a global or existential catastrophe to occur. Climate change has the potential to result in global catastrophic consequences at the more severe end of the possibility spectrum \u2013 e.g. 5 degrees and up (Wager & Weitzman, 2016). But we might also consider less severe climate change as a stressor, as it could be expected to lead to major droughts and famines and other resource shortages, mass migration, geopolitical tension that could result in local or global war, and so forth. It could also lead to international conflict, for example over the use of controversial mitigation techniques such as sulphate aerosol geoengineering technologies. \nMore generally, many of the specific risks we will look at need to be placed in the context of a world with a rising population, rising resource footprint, more extreme weather events, increasing pressures on ecosystem services, a changing physical and electronic infrastructure, and changing geopolitical pressures - and a world with a range of technologies more powerful than any we\u2019ve had in previous centuries. \nTopics of focus within the existential and global catastrophic risk research community \nRisks of particular focus within the existential risk community include those relating to our interaction with the global environment (for example, extreme climate change": null, " globally catastrophic biodiversity loss": null, " and risks from natural pandemic outbreaks). They also include risks from scientific and technological advances, such as risks from engineered pandemics and future advances in synthetic biology and other biotechnologies": null, " nuclear winter": null, " and risks from future advances in, and applications of, artificial intelligence. Some focus is given to risks from asteroids and supervolcanoes, but these have been less highly prioritised to date, due to the low frequency of occurrence of the relevant events": null, " the last global catastrophe-level asteroid impact event was the impact that created the Chicxulub Crater 66 million years ago, and the last likely global catastrophe-level supervolcano is thought to have been the Toba eruption 70,000 years ago (Ambrose, 2000). \nElectronic copy available at: https://ssrn.com/abstract": "3446663\n\nThe community also has research groups working on global systemic risks, interactions and cascades between risks, risks from future technologies such as neurotechnologies, advances in nanotechnology and other technologies. Researchers also focus on broader or more cross-cutting themes, such as governance challenges associated with global catastrophic risks, ethical considerations relating to the value of future generations, analysis of the costs and value of reducing existential and global catastrophic risk relative to other global priorities, foresight, horizon-scanning and road-mapping exercises for risks and relevant sciences and technologies, and methodological issues relating to reasoning about extreme events under great uncertainty. \nFor most individual global catastrophic and existential risks (with the possible exception of risk from advanced artificial intelligence), there are individual research communities working on these topics", " albeit sometimes focusing more so on risks at a lower end of the spectrum. Some have been doing so for decades. For example, there are many centres doing valuable work on nuclear non-proliferation and security, pandemic preparedness and surveillance, and bioweapon governance \u2013research leaders from these communities are represented at the Garrick Colloquium. There are numerous centres working on different aspects of climate change, biodiversity loss and other environmental risks and resource-related challenges. There are a number of excellent groups working on global systemic risks. NASA, the Planetary Defense community and others work on asteroid scanning and mitigation strategies. Furthermore there are a range of centres in academia, think tanks, government and elsewhere working on more cross-cutting issues such as risk governance and international security, foresight and scenario planning, and so forth. At a level below global catastrophic risk, there is excellent work on risk modelling being done in, and associated with, the reinsurance industry. \nThe existential and global catastrophic risk research community collaborates with, and draws on, research from these communities extensively. It aims to complement such work by looking at existential and global catastrophic risks as a class of risks, with the aim of: \n\u2022\tIdentifying the particular challenges associated with risks of this magnitude - whether they be \tscientific, analytic, ethical, or to do with governance, coordination, planning, or perception. \u2022\tIdentifying risk areas where insufficient attention has been paid to the most extreme scenarios. \u2022\tExamining how these different risks, and other global developments, may interact with each \tother. \n\u2022\tIdentifying previously unidentified or potential future risks \n\u2022\tTrying to distinguish which extreme scenarios are plausible and worthy of further work, even \tif they may be low probability, as opposed to those that can be dismissed as science fiction. \nRecent growth of this community \nThere has been a lot of recent growth within the existential risk research community. The first dedicated centre established was arguably Nick Bostrom\u2019s Future of Humanity Institute (FHI": null, " https://www.fhi.ox.ac.uk/) in Oxford in 2003. The FHI has focused on cross-cutting global \ncatastrophic risk analysis, philosophical analysis on the global importance of reducing existential risk, and in the last decade has placed its strongest focus on characterising potential risks from artificial general intelligence and superintelligence": null, " a lot of this has been in collaboration with the Machine Intelligence Research Institute in Berkeley. The Global Catastrophic Risk Institute (GCRI": null, " \nhttp://gcrinstitute.org/) in the US was founded in 2011, focusing on risks including bioweapons, nuclear war, artificial intelligence, and natural events, and employing risk analysis methodology. Under Seth Baum and Tony Barrett\u2019s leadership, it has played a key role in establishing links between the existential risk community and experts in these fields. \nElectronic copy available at: https://ssrn.com/abstract": "3446663\n\nThe Centre for the Study of Existential Risk (CSER", " http://cser.org) was founded in 2012 by Martin Rees, Huw Price, and Jaan Tallinn, although its first research grants were secured, and first \npostdoctoral researchers hired, more recently in late 2015 and 2016. We now have a research team beginning work on biological threats, extreme climate change, ecological tipping points, catastrophic risks related to future advances in artificial intelligence, and analysis of emerging technologies such as geoengineering. We also have postdocs working on more cross-cutting themes such as horizon-scanning and foresight for extreme risk, responsible innovation in risky sciences and technologies, population growth and resource use. \nThe Future of Life Institute (https://futureoflife.org/) was founded in 2014, focusing on artificial intelligence, climate change, risks from biotechnology, and risks from nuclear weapons. It has organised two highly successful conferences on the future of artificial intelligence and potential risks it might bring, resulting in a widely signed and shared open letter on the responsible development of AI, a grants programme to support work on AI safety, and a set of principles aimed at promoting the beneficial development and application of AI within the research community and more broadly. It has also organised a conference on nuclear war, and has engaged in activities to encourage divestment from nuclear weapons. \nSeveral more recent initiatives are underway within academia, including at Stockholm, Warwick (United Kingdom), and Australia National University, and other world-leading risk centres such as the Garrick Institute (UCLA) are increasingly including global catastrophic risk within their remit, indicating that a diverse range of new expertise will be brought to bear on these topics in coming years. \nWhat can we learn from this research, and what can we do with that knowledge? \nIt is worth considering the insights we can gain from studying risks of this magnitude that may not be gleaned from bodies of work analysing and mitigating risk more broadly, or risks of a lower magnitude. \nOne source of value is simply to be able to rule out scientifically implausible scenarios, in order to better focus our attention on existing threats and plausible future threats. This is of particular value for low-probability, extreme impact events, especially those that may be entirely novel. \nMany researchers argue the value of understanding existential risk on the grounds of their long-term moral significance. If we can demonstrate that certain threats have the potential for human extinction, or permanent collapse, then it is argued that the importance of mitigating them increases dramatically, due to the fact that they would not only harm current generations, but wipe out the potential for a huge number of future lives. \nThere may be specific actions we could take that are designed to mitigate the most severe versions of these threats in particular. These might include establishing a very strict ban on specific types of virus research or bioweapons, or on research on AI systems that could both come up with a realistic model of the world and engage in recursive self-improvement. They could also include seed banks, shelters and alternative foods, so that even in many otherwise extinction-level events we might increase the odds of continuation of the species. \nIn addition, there may be strategies we would avoid adopting unless we had reasonable cause to believe we were headed for a world in which a particular type of global catastrophe were likely. For example, it would be extremely foolish and unconstructive to call for a ban on artificial intelligence research at this point. At some point in the future however, a body of evidence might indicate that certain developments with catastrophic potential are likely within several years. This then might be \nElectronic copy available at: https://ssrn.com/abstract": "3446663\n\nreason to consider a temporary moratorium on progress to enhance the capability of AI systems, while various containment and safety measures were being explored. This is not without precedent, even at lower-levels of risk: the US White House issued a temporary moratorium on gain-of-function influenza research several years ago to allow time for more risk-benefit analysis (Lipsitch & Inglesby, 2014). \nOr consider a world in which evidence indicated we were headed for climate change of 5 degrees or over, in the absence of drastic action. In these circumstances we might consider deploying technologies, such as sulphate aerosol geoengineering, which we might be hesitant to deploy under less severe scenarios (Crutzen, 2006). Our reservations might be based on concerns over risks posed by the intervention itself, thorny global governance challenges that the intervention presents, or questions over public acceptance. These may be sound reservations that would rule out these strategies in all but the most exceptional of circumstances. A research community developing indicators that we may be approaching unusually dangerous global circumstances, and developing strategies for last-ditch solutions, may be of considerable value given the risks we may face in the coming century \u2013 even if many of these strategies are never needed or deployed. \nLastly, by studying the particular scientific, governance, ethical, and communication issues that arise when confronting one global catastrophic risk, we can learn valuable lessons to draw on for future challenges. For example, climate change in some ways exemplifies a lot of the issues that make existential and global catastrophic risk especially challenging. There is still a lot of scientific uncertainty over timelines, probability and pathways to the most severe impacts. The scale and impact are difficult to grasp, impossible to see, and the most severe consequences will fall on future generations. We still don\u2019t have broad public acceptance of the science, especially in the US. It involves countries around the world coordinating, each making near-term sacrifices in favour of the longer-term future. Yet the Paris Agreement was extremely encouraging. It involved 194 countries committing to make sacrifices in the interests of future generations in the face of these challenges. Despite the setback of the US\u2019s recent announcement of intent to withdraw, this remains an important achievement and a critical step towards progress on climate change. By learning from the successes and failures of this process, as well as from the history of nuclear non-proliferation and international diplomacy, norms and conventions prohibiting the use of biological weapons, and other global processes to manage and mitigate global risk, we can be better prepared for the future. \nKey challenges in existential and global catastrophic risk \nThere is a large body of expertise from the risk sciences that is of relevance to the study of existential and global catastrophic risk. However, a number of characteristics make these risks particularly challenging to analyse using normal risk analysis approaches. These include the difficulty in estimating the probability and expected impact of rare or even unprecedented events, where there may be sparse data to draw on", " and the changing nature of some risks, in particular those associated with rapidly developing technologies (especially those interacting with a rapidly developing infrastructure). \nFor some global catastrophic risks, probability is relatively straightforward to quantify. For asteroid impacts, for example, we can look to sources of evidence such as the earth\u2019s fossil record, patterns of impact craters on the Moon and on Mars, datasets of asteroids passing our field of vision, and use these to make a reasonable estimate of the frequency with which we might expect an asteroid of a given size to hit the earth. The pathways by which an asteroid impact would result in global catastrophe are also relatively straightforward": null, " therefore we can estimate the expected global harm expected from asteroids of different sizes. \nElectronic copy available at: https://ssrn.com/abstract": "3446663\n\nHowever, a similar analysis is much less straightforward for other risks. This can be illustrated by the example of catastrophic climate change. For a start, there is great uncertainty about the sensitivity of the earth system to the effects of our activities. The possibility of severe climate change is affected by factors including to but not limited to the potential for methane release from beneath the melting arctic permafrost, and from the seabed", " how much CO2 the deep ocean can absorb": null, " the possibility of collapse of Antarctic and Greenland ice sheets": null, " the possibility that the gulf stream might halt. The most worrying scenarios involve a combination of these factors driving each other as part of a positive feedback loop. While ongoing research will help us understand these factors and their interactions more clearly, it is very difficult to assign a meaningful probability to an outcome such as >5 degree climate change in the 21st century under a certain emission scenario. It may be that attempting to assign strict probabilities is the wrong approach": null, " an alternative would be to develop frameworks of \u2018safe operating thresholds\u2019 with wide error bounds, exemplified by the \u2018Planetary boundaries\u2019 framework put forward by Johan Rockstrom and colleagues at the Stockholm Resilience Centre (Steffen et al, 2015). The expected harm from long-term climate change will also depend very heavily on the extent to which various mitigation and adaptation strategies are adopted, and how successful they are. \nSimilarly, in the case of global pandemics, we we can ask scientific questions about the possibility of the \u2018perfect virus\u2019 with high health impact, high infectivity, and long incubation time. However, the scale and severity of impact will be predicated by many other factors \u2013 movement of humans or other vectors, capabilities of health services, the response of the population, and more. It is plausible that the bulk of the damage might not even be caused by the virus, but instead by a broad infrastructure collapse as emergency services and hospitals are overwhelmed, just-in-time food delivery is disrupted, and other systems underpinning societal order collapse. \nHowever, none of these challenges are insurmountable. Modelling and analysis of factors that contribute to these risks can deepen our scientific understanding. This can help us establish estimates for probability and impact, and in some cases rule out concerns entirely. Where past examples are sparse, there is value in drawing on counterfactual examples of \u2018near misses\u2019, as described by Gordon Woo (Woo, 2016) and others. Design and analysis of scenarios can help in identifying key considerations and interactions. This may help us identify key interventions that reduce risk significantly, even if in instances where it is difficult to assign tight probabilities to events. Other risk analysis techniques, and interventions to mitigate risks at scales smaller that global catastrophe level, also provide a range of useful insights for the analysis of global catastrophic risks, as shown by the work of GCRI and others. \nBy studying global catastrophic risks as a class of risks, we can identify shared characteristics of global catastrophe events, which may help us identify common strategies that aid us in becoming more resilient as a species against a broad set of risks. For example, a number of global catastrophic events (global nuclear war, supervolcano eruption, asteroid impact) would result in large amounts of particulate matter being ejected into the atmosphere, resulting in a disruption of photosynthesis (Maher and Baum, 2013). The development of alternative foods that are not dependent on sunlight, and strategies to scale up production of these food sources rapidly, would be robust in the face of a broad range of catastrophe events (Denkenberger and Pearse, 2014). The maintenance of permanent seed banks, manned shelters suitable for lengthy use, and information vaults represent similar safeguards. Similar strategies, useful for reduction of a broad range of risks, are likely to be feasible at the level of national and international governance (Farquhar et al, 2017": null, " Cotton-Barrett et al, 2016). \nElectronic copy available at: https://ssrn.com/abstract": null, " S., J., Dafoe, A., Zhang, B., Evans, O. (2017) When Will AI Exceed Human Performance? Evidencefrom AI Experts arXiv preprint arXiv::1705.08807 \nLipsitch, M., & Inglesby, T. V. (2014). Moratorium on research intended to create novel potential pandemicpathogens. MBio, 5(6), e02366-14.\nMaher, T. M., & Baum, S. D. (2013). Adaptation to and recovery from global catastrophe. Sustainability, 5(4),1461-1479.\nMillett, P. and Snyder-Beattie, A. (2017). Human Agency and Global Catastrophic Biorisks. Health security.\nElectronic copy available at: https://ssrn.com/abstract": null}